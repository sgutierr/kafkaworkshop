= Debezium and Red Hat Streams demo

This demo for Red Hat Streams (Kafka) will show how to replicate data and
data changes from a Postgres database with Debezium into a SQLServer db
using Kamelets.

image::images/debezium-postgresql-sqlserver.png[]

At the end of this Readme you will find a second part (optional) of this demo, where, in addition, we expose CDC events through a **secure** REST API to both external or internal client applications. This is the diagram including the second part:

image::images/debezium-3scale.png[]

== Prerequisites

To run this demo you need to have an OpenShift cluster up and running and
a user with sufficient rights on that cluster.

=== Setup Project `appdev-kafka`
Setup a namespace for this example, e.g. this demo uses "appdev-kafka":

[source,console]
----
oc new-project appdev-kafka --display-name="Debezium AMQ Streams"
----
=== Setup Red Hat Container Registry Secret

For downloading the AMQ Streams Kafka container image in the next step,
a Registry Service Account must be created on the Red Hat Customer Portal
in the https://access.redhat.com/terms-based-registry/[Registry Service Accounts]
section.

== Installation and Configuration

The demo consist of all these components:

* PostgreSQL: the source database.

* Debezium PostgreSQL Connector: changes from one database table are written to a Kafka topic whose name corresponds to the table name.

*  AMQ stream cluster: a real time event streaming platform.

*  Kafka Connect with Camel SQL connector: reads the changed data capture messages from Kafka topics and, according to the message type, it applies an action to the SQL server database.

* SQL Server: target database

* In addition, for monitoring Debezium together with all the components we deploy: 
**  Prometheus: provides an open source set of components for systems monitoring and alert notification.
**  Grafana: provides visualizations of Prometheus metrics.


=== Install Red Hat AMQ Streams (Strimzi Operator)

[NOTE]
====
In order to add Strimzi Operator the user needs to have right to add
CustomResourceDefinition objects.
====

[source,console]
----
oc apply -f 01-kafka-event-broker.yml -n appdev-kafka
----

For further installation options, like using the OpenShift OperatorHub,
look at https://access.redhat.com/documentation/en-us/red_hat_amq/2020.q4/html/deploying_and_upgrading_amq_streams_on_openshift/deploy-intro_str#con-streams-installation-methods_str[AMQ Streams installation methods].


Next command addresses an issue with the persisten volume creation (might be unnecessary).

[source,console]
----
oc adm policy add-scc-to-user anyuid system:serviceaccount:appdev-kafka:default
----

=== Setup demo databases
This demo uses the Debezium PostgreSQL Example database, and an empty SQL Server database as the destination.

==== PostgreSQL
The image used is from debezium repo, in particular a postgresql example with all debezium plugins for postgresql already installed: quay.io/debezium/example-postgres:1.9
The username and password are set as a parameter.  
[source,console]
----
oc apply -f 02-postgres-service.yml -n appdev-kafka
----

==== SQL Server
Port is 1433 and username and passwords can be customized as a container parameter. 
Find the deployment of the SQL Server container here: https://raw.githubusercontent.com/johwes/sqlworkshops-sqlonopenshift/master/sqlonopenshift/01_deploy/sqldeployment.yaml

[source,console]
----
oc apply -f 03-msqldeployment.yml -n appdev-kafka
----
Database client, two options:   

* Through port-forwarding option to access from your local: `+oc port-forward <msql_pod_name> 1433:1433+`

* Get into the pod and `/opt/mssql-tools/bin/sqlcmd -U sa -P Chang3-mssql-pwd!`

Then issue the follwing commands:

[source,console]
----
1> SELECT * FROM  INFORMATION_SCHEMA.TABLES
2> GO
----

Now it is time to create a database with the name `+dbo.order1+`, and also setup an orders table in that schema (use the sa user to create the new database and table and grant permissions):

`Order` table creation in SQL server:

[source,properties,title='create-table-order1.sql']
----
include::create-table-order1.sql[]
----

=== Start Debezium Postgres Source Connector

Finally, it's time to start the Debezium Postgres Source Connector.

The YAML config points to the previously created PostgreSQL instance.

And apply that config:

[source,console]
----
oc apply -f 04-debezium-postgresql-connect.yml -n appdev-kafka
oc apply -f 05-debezium-postgresql-connector.yml -n appdev-kafka
----

=== Check Messages in Managed Kafka

Now check if messages arrived in the Change Data Capture (CDC) Kafka topics
that were created by Kafka Connect, you can use Kafdrop or kafka-console-consumer.sh through the Kafka Connect pod:

[source,console]
----
oc exec -c kafka event-broker-dr-kafka-0 -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic inventory.inventory.orders --from-beginning
----

=== Enable Kamelets in your environment

IMPORTANT: To leverage *Kamelets*, you have to install the *Red Hat Integration - Camel K* Operator

1. Navigate to the *OperatorHub* (Operators -> OperatorHub)
2. Type `camel` in the Filter by keyword
3. Select on the *Red Hat Integration - Camel K* Operator
4. Click *Install*
5. Review the default options, and click *Subscribe*

The following step *add a user* in the Kafka configuration because the default Source Kafka Kamelet expects it to work properly.

[source,console]
----
oc apply -f 10-kuser.yaml -n appdev-kafka
----

The goal of the following command is to create an _Integration Platform_ in the target _namespace_, which is a prerequisite to leverage the predefined Kamelets and to define new ones.

Optionally, explore the https://camel.apache.org/camel-kamelets/0.8.x/index.html[kamelet catalog].

[source,console]
----
oc apply -f 11-integration-platform.yaml -n appdev-kafka
----

=== Connecting Kafka with Microsoft SQL Server

Kamelets (*Kamel* route snipp**ets**) allow users to connect to external systems via a simplified interface, hiding all the low level details about how those connections are implemented.

A Kamelet can act as *"source"* of data or alternatively as *"sink"*: a _source_ allows to consume data from an external system, while a _sink_ can let you send data to an external system or execute a particular action and get a result.

The user can leverage the *Kamelet* power with a little knowledge of their actual implementation by combining two existing Kamelets (a source and a sink) inside a *Kamelet Binding*.

The *Kamelet Binding* defines what are the source and the sink Kamelet and their specific runtime parameters. So in our specific usecase we should define:

- *Kafka Source* Kamelet with its own parameters (bootstrap URL, credentials, etc)
- *Microsoft SQL Server Sink* Kamelet with its own parameters (db name, credentials, etc)

Regrettably, the Microsoft SQL Server Kamelet is not able to manage the message coming from *Debezium* so we need to create a new one to perform the following tasks:

- Distinguish differents events coming from _Debezium_ (insert, delete and update)
- Accept 3 different queries to handle properly the incoming events
- Feed the correct payload (a JSON containing the records values)

For further details inspect the link:12-debezium-sqlserver-sink.yaml[Kamelet definition].

Create the new _Debezium aware_ Microsoft SQL Server Kamelet:

[source,console]
----
oc apply -f 12-debezium-sqlserver-sink.yaml -n appdev-kafka
----

Finally, create the *Kamelets binding* to bind together _Kafka_ and _Microsoft SQL Server_:

[source,console]
----
oc apply -f 13-kafka-mssql-binding.yaml -n appdev-kafka
----

Wait until the Kamelet bindings gets `READY`: 

[source,console]
----
$ oc get -w kameletbinding
NAME                  PHASE   REPLICAS
kafka-mssql-binding   Ready   1
----

A new pod is started executing the integration logic (e.g.: `kafka-mssql-binding-8c6db8d75-h9tcp`)

=== Validating Data Arrived in SQL Server

Now connect to the SQL instance and check if data successfully
arrived in the replicated table:

[source,sql]
----
SELECT * FROM order1;

# +-------+--------------------+-------------+------------+-------------------+
# | id    |    order_date      | purchaser   | quantity   |   product_id      |
# +-------+--------------------+-------------+------------+-------------------+
# | 10001 |    2016-01-31      |    1001     |     1      |       102         |
# | 10002 |    2016-01-31      |    1002     |     1      |       105         |
# | 10003 |    2016-01-31      |    1002     |     1      |       106         | 
# | 10004 |    2016-01-31      |    1003     |     1      |       107         |
# +-------+--------------------+-------------+------------+-------------------+
# 4 rows in set (0.01 sec)
----

Now connect to the SQL instance and check if data successfully
arrived in the replicated table:

== Monitoring

*This demo will use Prometheus Operator and Grafana Operator to deploy them and monitor the AMQ Streams ecosystem easily.*

Note: The Kafka and Debezium metrics configurations are already created when you applied 01-kafka-event-broker.yml and 04-debezium-postgresql-connect.yml scripts.

=== Installing Prometheus and Grafana Operators (install both)
1. Navigate to the OperatorHub (Operators -> OperatorHub)
2. Type prometheus / grafana in the Filter by keyword
3. Select on the Prometheus / Grafana Operator
4. Review the Community Operator notice, and click Continue
5. Click Install
6. Review the default options, and click Subscribe

=== Grafana Configuration

1.Set up the prometheus HTTP URL: http://prometheus-operated.appdev-kafka.svc:9090 into the Prometheus data source.

image::images/datasources-prometheus.png[]

2.Import Grafana dashboards:
**** 
. Go to "Create (Dashboard)" option -> import and click on the "upload json" button
.. **Debezium** select debezium-dashboard.json file included in this repo.
.. **Strimzi** find the dashboards json files in this repo: https://github.com/strimzi/strimzi-kafka-operator/tree/main/examples/metrics/grafana-dashboards 
****

3.Go to the Debezium dashboard and set these parameters as follows to run:
****
. Connector Type: **postgres**
. Connector Name: **inventory**
****
image::images/grafana-debezium.png[]

== SECOND PART: Adding HTTP REST API and Protection  (Optional)

This section exposes CDC events through a **secure** REST API to both external or internal client applications.
**Prerequisites**: 3scale deployed on the same Debezium OpenShift cluster.

=== Kafka HTTP Bridge 
HTTP Bridge and Route creation:
[source,console]
----
oc apply -f 08-kafkabridgehttp.yml -n appdev-kafka
----

=== Kafka HTTP Bridge / 3scale Integration

3scale enables security and protection to the REST API provided by Kafka HTTP bridge, now you can monitor API traffic and expose the API to external consumers in a secure way. 

When Kafka HTTP Bridge and 3scale are running in the same OpenShift cluster you can use the **3scale service discovery** feature, which it is possible to scan for discoverable API services that are running in the same OpenShift cluster and automatically import the associated API definitions into 3scale.

To discover the HTTP Bridge service with 3scale when it is deployed in a different namespace. There are two options depending on whether you are allowed to view a specific OpenShift user or the 3scale service account:

**** 
. Give permission to a specific 3scale/OpenShift user:  
https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.6/html/admin_portal_guide/service-discovery

. Give permission to the 3scale service account. In order to add view permissions to the service account amp you need to use the following command, noticing the syntax which references the scc is: “system:serviceaccount:namespace:serviceaccountname”. 
    The 3scale service account is called amp and the 3scale namespace in my environment is appdev-apimanager
[source,console]
----
    oc adm policy add-role-to-user view system:serviceaccount:appdev-apimanager:amp -n appdev-kafka
    oc rollout latest dc/system-app
----
****

=== 3scale configuration

==== Import the Kafka HTTP API
After giving permissions, 3scale is able to discover the Kafka HTTP API. Just signing in 3scale Administration console, create new product and choosing "Import from OpenShift". You will see the service successfully imported in 3scale. 

image::images/3scale-discovery.png[]

==== Create a 3scale Application Plan
For publishing an API in 3scale you must configure at least an Application Plan with their rate limits if this is a requirement.

image::images/3scale-application-plan.png[]

==== Create an Application Client 
In order to obtain the credential to access the API, create a 3scale Application through Accounts --> choose an Account --> Click on "New Application" button.

image::images/3scale-create-application.png[]

After this, you have to save the credential generated for testing ( YOUR-3SCALE-CREDENTIALS )

==== Getting the 3SCALE-GATEWAY-HTTP-BRIDGE-URL
Go to the HTTP bridge service (i.e my-bridge-bridge-service)  --> Integration --> Configuration and get the URL from Staging APIcast or Production APIcast whatever you prefer.

=== Testing

Take a look at this article to understand how the Kafka HTTP bridge works: 
https://strimzi.io/blog/2019/07/19/http-bridge-intro/

NOTE: replace these parameters in the following curl calls: 
  . *3SCALE-GATEWAY-HTTP-BRIDGE-URL* by the 3scale URL specified in the 3scale Kafka HTTP bridge service configuration
  . *YOUR-3SCALE-CREDENTIALS* by your 3scale credentials obtained from the 3scale application.

==== 1. Consumer subscription

This calls creates a consumer called "consumer1" belonging to the "cdc-demo-consumer-group" group.

[source,console]
----
curl --location --request POST 'https://3SCALE-GATEWAY-HTTP-BRIDGE-URL/consumers/cdc-demo-consumer-group?user_key=YOUR-3SCALE-CREDENTIALS' \
--header 'Accept: application/vnd.kafka.v2+json' \
--header 'Content-Type: application/vnd.kafka.v2+json' \
--data-raw '{
    "name": "consumer1",
    "format": "json",
    "auto.offset.reset": "earliest",
    "enable.auto.commit": false,
    "fetch.min.bytes": 512,
    "consumer.request.timeout.ms": 30000,
    "isolation.level": "read_committed"
}'
----

==== 2. Topics subscription

[source,console]
----
curl --location --request POST 'https://3SCALE-GATEWAY-HTTP-BRIDGE-URL/consumers/cdc-demo-consumer-group/instances/consumer1/subscription?user_key=YOUR-3SCALE-CREDENTIALS' \
--header 'Accept: application/vnd.kafka.v2+json' \
--header 'Content-Type: application/vnd.kafka.v2+json' \
--data-raw '{
    "topics": [
       "inventory.orders",
       "inventory.inventory.products",
       "inventory.inventory.customers"
    ]
}'
----

==== 3. Get CDC records

NOTE: you need to call many times before obtaining a result otherwise you see something like "..topics:[]"
Read these links to understand why:
https://github.com/strimzi/strimzi-kafka-bridge/issues/520 
https://access.redhat.com/documentation/en-us/red_hat_amq_streams/2.1/html/using_the_amq_streams_kafka_bridge/assembly-kafka-bridge-quickstart-bridge

[source,console]
----
curl --location --request GET 'https://3SCALE-GATEWAY-HTTP-BRIDGE-URL/consumers/cdc-demo-consumer-group/instances/consumer1/records?user_key=YOUR-3SCALE-CREDENTIALS' \
--header 'Accept: application/vnd.kafka.json.v2+json'
----

Example using Postman:

image::images/3scale-postman-test.png[]

